[["index.html", "oepsData Package Documentation Chapter 1 Introduction 1.1 In this documentation 1.2 Other useful links 1.3 Releases", " oepsData Package Documentation Chapter 1 Introduction The Opioid Environment Policy Scan (OEPS) is an open-source data warehouse created by the Healthy Regions &amp; Policies Lab to support researchers in studying and modeling the opioid risk environment. This website is intended as a starting place for researchers interested in using the OEPS data, especially using R. 1.1 In this documentation Getting started How to install the package Usage Explanation of available functions Examples Some quick examples of how oepsData can be used Getting OEPS Data from BigQuery How to access OEPS data in Google BigQuery While the examples demonstrate usage of the R package bigrquery, this section will be helpful regardless of which client libary you use to access BigQuery. 1.2 Other useful links healthyregions/oepsData Repository for this R package. Please record bugs or feature requests in the repo issues OEPS Explorer The main project website for OEPS healthyregions/oeps The main repository for the OEPS Explorer frontend app and backend ETL app 1.3 Releases v0.1.0 December 2024 - Initial release. "],["getting-started.html", "Chapter 2 Getting started 2.1 Installation", " Chapter 2 Getting started 2.1 Installation Installing oepsData is easy. Just run the following command to grab the newest release from GitHub. install.packages(&#39;devtools&#39;) devtools::install_github(&#39;https://github.com/healthyregions/oepsData@v0.1.0&#39;) Or, to install the latest development version (which may be unstable), omit the release tag: devtools::install_github(&#39;https://github.com/healthyregions/oepsData&#39;) You can then load the package like any other R package: library(oepsData) Efforts are currently under way to list the package on CRAN. "],["usage.html", "Chapter 3 Usage 3.1 load_oeps_dictionary 3.2 load_oeps 3.3 state_to_fips 3.4 county_to_fips", " Chapter 3 Usage oepsData is centered around two functions: load_oeps_dictionary, which loads a basic data dictionary; and load_oeps, which directly loads OEPS data. We expect that most users will start by calling load_oeps_dictionary to look at what data is available at their desired analysis scale, followed by calling load_oeps to actually load the data. 3.1 load_oeps_dictionary load_oeps_dictionary takes one argument: scale One of “tract”, “zcta”, “county”, or “state” It returns the data dictionary (stored as a data.frame). # Load all data available at the state level data_dictionary &lt;- load_oeps_dictionary(scale=&quot;state&quot;) If you are working in RStudio, we recommend browsing the dictionary through the View command: View(data_dictionary) Here in the docs we can preview it directly: data_dictionary &lt;- load_oeps_dictionary(scale=&quot;state&quot;) data_dictionary 3.2 load_oeps We might find that we’re interested in the 1990 state data. We can load that data and its geometries using load_oeps, which accepts the following arguments: scale The scale of analysis. One of “tract”, “zcta”, “county”, or “state” year The release year for the data. One of 1980, 1990, 2000, 2010, or 2018. themes The theme to pull data for. One of ’Geography”, “Social”, “Environment”, “Economic”, “Policy”, “Composite”, or “All”. Defaults All. states A string or vector of strings specifying which states to pull data for, either as FIPS codes or names. Ignored when scale is in ZCTA. Defaults None. counties A string or vector of strings specifying which counties to pull data for, either as FIPS or names. Ignored for ZCTA, and must be specified alongside states. Defaults None. tidy Boolean specifying whether to return data in tidy format; defaults to FALSE. geometry Boolean specifying whether to pull geometries for the dataset. Defaults FALSE cache Boolean specifying whether to use cahced geometries or not. Defaults TRUE. See A note on caching for more information. states_1990 &lt;- load_oeps(scale=&quot;state&quot;, year=1990, geometry=TRUE) head(data.frame(states_1990)) Which lets us operate on the data as we desire. For instance, we can make a simple map: library(tmap) #&gt; Breaking News: tmap 3.x is retiring. Please test v4, e.g. with #&gt; remotes::install_github(&#39;r-tmap/tmap&#39;) library(sf) #&gt; Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE # reproject to a better display CRS states_1990 &lt;- st_transform(states_1990, &quot;ESRI:102004&quot;) tm_shape(states_1990) + tm_fill(&quot;NoHsP&quot;, style=&quot;jenks&quot;) + tm_borders(alpha=0.05) + tm_layout(main.title = &quot;Population over 25 without a high school degree&quot;) See Examples for many more demonstrations of how you can use this function. 3.2.1 A note on caching oepsData pulls its data from online repositories, primarily GitHub. This can lead to issues for users operating on slow internet, for whom load times can be long for larger datasets, or for users who anticipate needing the package when entirely offline. To help minimize these issues, oepsData caches, or saves a local copy of, data loaded by load_oeps on its first load. Any later usage of the dataset will be pulled from the local cache. Additionally, oepsData offers a few commands can help maintain caches: cache_geometries and cache_oeps_tables will pre-cache all tables and geometries (it will overwrite existing cache content in the process). clear_cache deletes all cached data. cache_dir returns the directory of the oepsData cache. Users who want to avoid using cached data and instead download data fresh every time can set cache=FALSE when calling load_oeps. 3.3 state_to_fips This is a helper function that takes a given state’s name or abbreviation and returns the state’s FIPS code. state_to_fips(&quot;Illinois&quot;) #&gt; [1] &quot;17&quot; state_to_fips(&quot;IL&quot;) #&gt; [1] &quot;17&quot; 3.4 county_to_fips This is a helper function that takes a county’s name and the FIPS of its state and returns the county level FIPS code. county_to_fips(&quot;Champaign&quot;, 17) #&gt; [1] &quot;17019&quot; county_to_fips(&quot;Champaign&quot;, state_to_fips(&quot;IL&quot;)) #&gt; [1] &quot;17019&quot; "],["examples.html", "Chapter 4 Examples 4.1 Data subsetting 4.2 Longitudinal analysis 4.3 Combining outside data", " Chapter 4 Examples 4.1 Data subsetting load_data offers a few tools to help subset data on first load. To get a better sense of how these tools can be put towards practical use, let’s walk through a quick and dirty spatial investigation of poverty and working age population in Chicago. We start by loading the data dictionary to see what data is available and into what themes they are categorized. This lets us request a subset of the full data set, which minimizes the amount of data in our environment at a time. # See what data is available data_dictionary &lt;- load_oeps_dictionary(scale=&#39;tract&#39;) # if working in RStudio, we recommend: # View(data_dictionary) data_dictionary Based on the data dictionary, we can see that we have two variables of interest – Age18_64 and PovP. Both variables are available at multiple time periods, and they belong to different themes – social and economic, respectively. Let’s pull our data using load_oeps. Note how many parameters we provide information for. Geographically, we provide the FIPS code for our desired states (just Illinois) and counties (just Cook), which dramatically cuts down the number of entries retrieved. Additionally, we provide our variables of interests’ themes, lowering the number of rows retrieved. # Grab the data cook_county_2010 &lt;- load_oeps( scale=&#39;tract&#39;, year=&#39;2010&#39;, themes=c(&#39;social&#39;, &#39;economic&#39;), states=&#39;17&#39;, counties=&#39;031&#39;, geometry=T) # Preview what we got head(data.frame(cook_county_2010)) We can then immediately map our data. We opt to use tmap in this example, but ggplot2 also has mapping functionality for users more familiar with the library. library(tmap) tm_shape(cook_county_2010) + tm_fill(c(&#39;Age18_64&#39;, &#39;PovP&#39;), title = c(&#39;Working Age&#39;, &#39;Poverty\\nPercentage&#39;), style = c(&#39;sd&#39;, &#39;sd&#39;), palette = &#39;BrBG&#39;) + tm_layout(legend.position = c(&#39;left&#39;, &#39;bottom&#39;), frame=FALSE) Based on the above, we observe that fewer working age individuals correlates roughly with higher poverty rates within the southern and western sides of Chicago, with this trend breaking down in northern suburbs. Chicagoland continues a little bit into northeastern Indiana; maybe the rough correlation is present there as well? To check, we can pass a list of county GEOIDS to the counties parameter of load_oeps. Note that these GEOIDS must consist of the state FIPS and county FIPS, as they would otherwise fail to uniquely identify our two counties. # Grab the data chicago_metro_2010 &lt;- load_oeps( scale=&#39;tract&#39;, year=&#39;2010&#39;, theme=c(&#39;social&#39;, &#39;economic&#39;), counties=c(&#39;17031&#39;, &#39;18089&#39;), geometry=T) # Preview what we got head(data.frame(cook_county_2010)) We can once again map the resultant data, and find that the rough correlation between lower working age population totals and higher poverty in more densely populated, coastal areas seems to hold for the broader Chicagoland area. library(tmap) tm_shape(chicago_metro_2010) + tm_fill(c(&#39;Age18_64&#39;, &#39;PovP&#39;), title = c(&#39;Working Age&#39;, &#39;Poverty\\nPercentage&#39;), style = c(&#39;sd&#39;, &#39;sd&#39;), palette = &#39;BrBG&#39;) + tm_layout(legend.position = c(&#39;left&#39;, &#39;bottom&#39;), frame=FALSE) 4.2 Longitudinal analysis The oepsData package also enables easier longitudinal analysis. Continuing our above example, we might be interested in the change in percent poverty over time throughout Chicagoland. To check, let’s compare 2000 data to 2010 data. We start by grabbing data from 2000: # get new data chicago_metro_2000 &lt;- load_oeps( scale=&#39;tract&#39;, year=&#39;2000&#39;, theme=c(&#39;social&#39;, &#39;economic&#39;), counties=c(&#39;17031&#39;, &#39;18089&#39;), geometry=F) Because variables have the same column name in every year, chicago_metro_2000 and chicago_metro_2010 have columns with the same names. To fix this, we need to select and rename som eof our columns; there are a wide variety of possible approaches to this problem, but we opt to use dplyr to select and rename our columns of interest. # rename data columns chicago_metro_2000 &lt;- dplyr::select(chicago_metro_2000, &quot;HEROP_ID&quot;, &quot;PovP2000&quot;=&quot;PovP&quot;) chicago_metro_2010 &lt;- dplyr::select(chicago_metro_2010, &quot;HEROP_ID&quot;, &quot;PovP2010&quot;=&quot;PovP&quot;) We can then merge the dataframes. Two merge keys are provided with oepsData: a HEROP specific merge-key called HEROP_ID and the more common GEOID. We recommend merging on HEROP_ID when merging HEROP data, and reserving GEOID for compatability with outside datasets. # we changed the name of our merge keys earlier # so we resolve that here. chicago_metro_longitudinal &lt;- merge(chicago_metro_2010, chicago_metro_2000, by.x=&#39;HEROP_ID&#39;, by.y=&#39;HEROP_ID&#39;) With longitudinal data in hand, we can then conduct basic analyses on our data. For instance, we can make side-by-side maps of poverty rates: tm_shape(chicago_metro_longitudinal) + tm_fill(c(&#39;PovP2000&#39;, &#39;PovP2010&#39;), title = c(&#39;2000 Poverty\\nPercentage&#39;, &#39;2010 Poverty\\nPercentage&#39;), style = c(&#39;sd&#39;, &#39;sd&#39;), palette = &#39;BrBG&#39;) + tm_layout(legend.position = c(&#39;left&#39;, &#39;bottom&#39;), frame=FALSE) Or calculate the change in poverty: # make a new variable chicago_metro_longitudinal &lt;- dplyr::mutate(chicago_metro_longitudinal, ChangePovP = PovP2010 - PovP2000) tm_shape(chicago_metro_longitudinal) + tm_fill(&#39;ChangePovP&#39;, title = &quot;Change in Percent\\n Poverty, 2000 to 2010&quot;, style = &#39;jenks&#39;, palette = &#39;BrBG&#39;) + tm_layout(legend.position = c(&#39;left&#39;, &#39;bottom&#39;), frame=FALSE) #&gt; Variable(s) &quot;ChangePovP&quot; contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette. 4.3 Combining outside data In this section, we’ll demonstrate how to pair oepsData with outside data sources to glean new insights through a short case study. Evelyn is a public policy researcher that wants to investigate the viability of two interventions to increase healthcare accessibility in her region: increased transit availability through rideshare vouchers, and increased telehealth outreach. Through a partnership with the local regional hospital, she managed to survey 110 of the hospital’s patients, getting data on how they use the facility and their access to car transit. Let’s see how the oepsData package can help Evelyn contextualize her data and create an initial hypothesis about where best to target these interventions. Evelyn starts by using the load_oeps_dictionary command to view the variables available at the ZCTA scale. She identifies two relevant variables for her purposes: FqhcTmDr, or the drive time to the nearest Federally Qualified Healthcare Facility, and MhTmDr, or the drive time to the nearest mental health facility. Both variables are under the Environment theme, so she uses load_oeps to pull the data: library(oepsData) library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union zcta &lt;- oepsData::load_oeps(scale=&#39;zcta&#39;, year=&#39;2018&#39;, theme=&#39;Environment&#39;, geometry=TRUE) zcta &lt;- zcta[c(&#39;GEOID&#39;, &#39;FqhcTmDr&#39;, &#39;MhTmDr&#39;)] dplyr::glimpse(zcta) #&gt; Rows: 32,990 #&gt; Columns: 4 #&gt; $ GEOID &lt;int&gt; 1001, 1002, 1003, 1005, 1007, 1008, 1009, 1010, 1011, 1012, 1… #&gt; $ FqhcTmDr &lt;dbl&gt; 8.04, 0.00, 3.27, 29.70, 15.10, 18.59, 24.86, 17.61, 20.80, 1… #&gt; $ MhTmDr &lt;dbl&gt; 8.04, 0.00, 0.00, 0.00, 15.10, 18.59, 7.56, 15.87, 34.72, 18.… #&gt; $ geometry &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-72.66768 4..., MULTIPOLYGON (((… Evelyn then filters down to the ZCTAs in her region. Depending on what data Evelyn has on hand, she has a few possible approaches. If she has a geometry for her region, she can use a spatial filter through sf: library(sf) # Read in data study_region &lt;- st_read(&#39;data/service_area.shp&#39;) #&gt; Reading layer `service_area&#39; from data source #&gt; `/Users/runner/work/oepsData/oepsData/data/service_area.shp&#39; #&gt; using driver `ESRI Shapefile&#39; #&gt; Simple feature collection with 1 feature and 1 field #&gt; Geometry type: POLYGON #&gt; Dimension: XY #&gt; Bounding box: xmin: -79.22458 ymin: 38.20665 xmax: -78.48574 ymax: 38.8501 #&gt; Geodetic CRS: NAD83 # Match CRS crs &lt;- &quot;EPSG:3968&quot; zcta &lt;- st_transform(zcta, crs) study_region &lt;- st_transform(study_region, crs) # Find ZCTAs which intersect the study region zcta &lt;- st_filter(zcta, study_region) Alternatively, if she has a list of relevant ZCTAs on hand, she can directly filter on those: # Read in list of relevant ZCTAs relevant_zctas &lt;- read.csv(&#39;data/zctas.csv&#39;)$x # Filter against the relevant ZCTAs zcta &lt;- zcta[zcta$GEOID %in% relevant_zctas,] Regardless of which approach she picks, her final result looks something like this: library(tmap) tm_shape(zcta) + tm_fill(col=&#39;FqhcTmDr&#39;, palette=&#39;Reds&#39;) + tm_borders() With the ZCTA data in hand, Evelyn can load in her survey data and attach it. survey &lt;- read.csv(&#39;data/survey_data.csv&#39;) head(survey) Evelyn’s data details the home ZIP of each individual (ZCTA), whether they utilize primary care services at the local medical facility (Has_Pcp), utilize specialists at the facility (Has_Sp), receive financial aid to help cover their medical expenses (Fin_Aid), and have access to a car (Has_Car). She’s interested in aggregate statistics, so instead of merging her data directly into the sample, she first aggregates it by ZCTA. survey &lt;- survey |&gt; select(-RID) |&gt; group_by(ZCTA) |&gt; summarize(Fin_Aid = sum(Fin_Aid), # count who receive financial aid Has_Pcp = sum(Has_Pcp), # count who see a pcp Has_Sp = sum(Has_Sp), # count who see a specialist Has_car = sum(Has_Car), # count with a car Num_Resps = n()) # count of respondents # Full join, as not every ZCTA has a respondent zcta &lt;- merge(zcta, survey, by.x = &quot;GEOID&quot;, by.y = &quot;ZCTA&quot;, all=TRUE) head(data.frame(zcta)) With her data merged, Evelyn starts a three step preliminary analysis. First, she checks the distribution of her responses: tm_shape(zcta) + tm_fill(&#39;Num_Resps&#39;, style=&#39;cont&#39;, palette=&#39;RdPu&#39;, alpha=.8) + tm_borders() The majority of responses are from the center of her study area, near the city containing her partner medical facility, or from surrounding towns. Additionally, Evelyn doesn’t have respondents from West Virginia ZCTAs whose closest medical center is likely her partner facility. With this info in hand, Evelyn asks an her main question: which ZCTAs would most benefit from increased transit services to and from the local Methadone Clinic and Mental Health providers, and which areas would benefit more from increased telehealth opportunities and outreach? As a rough and tumble operationalization, she guesses that areas within a 30 minute drive of their nearest Mental Health provider and Methadone clinic but which have a below median percentage of patients with cars likely benefit most from a increased transit options, while locations further out might with fewer cars likely benefit most from increased telehealth. To map this, she does a quick bit of data wrangling: # What percentage of patients in a given ZCTA have cars? zcta[&quot;Has_CarP&quot;] &lt;- zcta[&quot;Has_car&quot;] / zcta[&quot;Num_Resps&quot;] * 100 #&gt; Warning in `[&lt;-.data.frame`(`*tmp*`, &quot;Has_CarP&quot;, value = structure(list(: #&gt; provided 2 variables to replace 1 variables med_car_pct &lt;- median(zcta$Has_CarP, na.rm = T) # Which areas would benefit most from increased transit option? zcta[&quot;More_Transit&quot;] &lt;- (zcta$MhTmDr &lt;= 30) &amp; (zcta$FqhcTmDr &lt;= 30) &amp; (zcta$Has_CarP &lt; med_car_pct) # Which areas would benefit most from increased telehealth? zcta[&quot;More_Telehealth&quot;] &lt;- (zcta$MhTmDr &gt; 30) &amp; (zcta$FqhcTmDr &gt; 30) &amp; (zcta$Has_CarP &lt; med_car_pct) head(data.frame(zcta)) Evelyn then plots the results, highlighting the borders of the ZCTAs she believes may benefit most from increased telehealth in blue, and zctas she believe may benefit most from increased transit in red. tm_shape(zcta) + tm_fill(col=&#39;pink&#39;, alpha=.25) + tm_shape(filter(zcta, More_Transit)) + tm_fill(lwd=3, col=&#39;red&#39;) + tm_shape(filter(zcta, More_Telehealth)) + tm_fill(lwd=3, col=&#39;blue&#39;) + tm_shape(zcta) + tm_borders(lwd=.2) Evelyn has thus identified six ZCTAs she thinks may benefit most from increased transit options, and one that she thinks will benefit more from increased telehealth accessibility. From here, she can and probably should continue to validate her results through multiple approaches. Computationally, she may want to check the percent of households without internet in each ZCTA – contained in the NoIntP variable she can get through another use of load_oeps – or collect increased responses to ensure her calculated percentages are accurate. Qualitatively, she may also want to validate her results against interview data from patients, to ensure that her proposed interventions agree with the needs expressed by the communities they will effect. "],["getting-oeps-data-from-bigquery.html", "Chapter 5 Getting OEPS Data from BigQuery 5.1 Overview 5.2 Setting up BigQuery 5.3 The low-level API 5.4 The dplyr API", " Chapter 5 Getting OEPS Data from BigQuery Opioid Environment Policy Scan data is also available on Google BigQuery. In this notebook, we’ll go over how to interact with the data using bigrquery. We go over two of the bigrquery APIs – one for readers familiar with SQL, and one for readers who want to avoid SQL. Lastly, readers who are already familiar with Google BigQuery will likely want to skip to Make a Query. 5.1 Overview When making queries against a BigQuery dataset, we do not directly query the dataset. Instead, we connect to a BigQuery profile and submit a job, which tells the profile to make the query in our stead and return the data. You can think of this like connecting to another computer to middleman the exchange. The setup allows users to work with multiple BigQuery datasets within a single profile, and also allows for billing to be separated so that data providers only pay to store the data instead of also paying for all usage of their data. The OEPS data warehouse itself is named oeps-391119 on BigQuery, and is divided into two datasets: tabular and spatial. The tabular dataset consists of 16 tables of attribute data at the state, county, tract, and ZCTA scales from 1980 to 2020. The spatial dataset contains the 2010 TIGER/Line geometries for each of these scales. The primary key for the datasets are HEROP_ID. A full dataset schema can be found on the OEPS BigQuery reference linked here. 5.2 Setting up BigQuery You can set up BigQuery for usage in R in three broad steps: Enabling BigQuery on your Google Account Grabbing the name of your BigQuery resource Connecting everything to BigRQuery Let’s start at step one. Sign into a Google account on your browser of choice before navigating to this link, where you will be prompted to “Enable BigQuery.” Do so to enable your account to access BigQuery and data through BigQuery. Once BigQuery is enabled, you’ll be taken to the BigQuery studio page. This page is a hub for BigQuery interaction on the cloud, and technically also a place from which you can test out SQL queries and manage connections to external databases. In the BigQuery diagram, it’s the computer on the cloud that you submit jobs to. For our purposes, we’re interested in the resources under the explorer. By default, Google creates a default resource by mashing together random words and numbers. You can proceed using this resource, or create a new, more memorably named resource through the “+ ADD” button at the top of the Explorer pane. Whichever route you take, we need to store the name of your BigQuery project in a variable for use. As it’s the project that gets billed for the queries, it’s conventional to refer to this project as “billing.” billing &lt;- &quot;oeps-tutorial&quot; # replace this with your project name! “Will I be charged money for using BigQuery?” It’s unlikely outside of unreasonably intense usage of BigQuery. As of September 2024, the free tier for BigQuery allows for 1 TiB of data querying. The entire OEPS dataset is less than 1 GiB in size, so you would need to pull the entire dataset over 1,000 times in a month to leave the free tier. Unless you have an automated pipeline pulling from OEPS or are pulling other datasets on BigQuery, this is a hard limit to reach! For more on BigQuery billing, see the Google BigQuery pricing page. Lastly, we need to establish that we actually have permission to create jobs on the account we created. To do that, we can use bigrquery::bq_auth(), and then grant the Tidyverse API a few permissions on our Google Account. Note that this command will prompt you to open a new window in your browser. # Opens your browser to authenticate your account bigrquery::bq_auth() Now that bigrquery is set up, we can explore using two of its interfaces to interact with BigQuery: the low-level API that uses SQL, and a higher level API using dplyr. 5.3 The low-level API The low-level API offers a series of methods that can be used to interact with BigQuery’s REST API. While bigrquery offers quite a few commands, it’s usually sufficient to use two: bq_project_query and bq_table_download. Using these commands, we can create and submit SQL queries to pull data tables from the OEPS data warehouse: library(bigrquery) # Our query sql &lt;- &#39;SELECT HEROP_ID, TotPop, PovP FROM oeps-391119.tabular.C_1990&#39; # Submit a job to grab the data tb &lt;- bq_project_query(billing, query=sql) # Download the results of that query to our system results &lt;- bq_table_download(tb) head(results) We can also use more complex queries: sql &lt;- &#39; SELECT C_1990.HEROP_ID, (C_2000.PovP - C_1990.PovP) AS ChangeInPovP, (C_2000.TotPop - C_1990.TotPop) AS ChangeInPop FROM oeps-391119.tabular.C_1990 INNER JOIN oeps-391119.tabular.C_2000 ON C_1990.HEROP_ID=C_2000.HEROP_ID &#39; tb &lt;- bq_project_query(billing, sql) results &lt;- bq_table_download(tb) head(results) If we want to plot this data, we need to query the spatial database. This is doable, but R interactive environments are not always a fan of the result, so we’re forced to turn results into an sf object before attempting to preview it. That is, for the following setup: library(sf) sql &lt;- &#39;SELECT HEROP_ID, geom FROM oeps-391119.spatial.counties2010&#39; tb &lt;- bq_project_query(billing, sql) this breaks: # bad results &lt;- bq_table_download(tb) head(results) And this works: # good results &lt;- bq_table_download(tb) |&gt; st_as_sf(wkt=&#39;geom&#39;, crs=&#39;EPSG:4326&#39;) # convert geom to sf head(results) 5.3.1 A full low-level pipeline: Putting this all together, we can create a quick map of how county level poverty changed from 1990 to 2000: library(tmap) tmap_mode(&#39;view&#39;) sql &lt;- &#39; SELECT C_1990.HEROP_ID, (C_2000.PovP - C_1990.PovP) AS ChangeInPovP, counties2010.name, counties2010.geom FROM oeps-391119.tabular.C_1990 INNER JOIN oeps-391119.tabular.C_2000 ON C_1990.HEROP_ID=C_2000.HEROP_ID INNER JOIN oeps-391119.spatial.counties2010 ON C_1990.HEROP_ID=counties2010.HEROP_ID &#39; tb &lt;- bq_project_query(billing, sql) results &lt;- bq_table_download(tb) |&gt; st_as_sf(wkt=&#39;geom&#39;, crs=&#39;EPSG:4326&#39;) tm_shape(results) + tm_fill(&#39;ChangeInPovP&#39;, style=&#39;sd&#39;, midpoint=0, title=&#39;Change in Poverty\\n 1990 to 2000&#39;, palette=&#39;-RdYlBu&#39;) 5.4 The dplyr API For users with less SQL familiarity, it’s also possible to use dplyr to interact with BigQuery. We’ll need the help of DBI, a library for interacting with databases in R. library(dplyr) library(DBI) library(bigrquery) For this pipeline, we use DBI to connect to a given dataset (e.g. tabular), before picking a table within the dataset to interact with and then manipulate that table using dplyr. # Connect to the tabular dataset conn &lt;- dbConnect( bigquery(), project = &#39;oeps-391119&#39;, dataset = &#39;tabular&#39;, billing = billing ) # List off available tables dbListTables(conn) We can then pick a table from the above and interact with it using dplyr. C_1990 &lt;- tbl(conn, &#39;C_1990&#39;) C_1990 |&gt; select(HEROP_ID, TotPop, PovP) |&gt; collect() |&gt; head() As with the low-level API, we can also do more complex tasks, albeit with a few more lines of code: C_1990 &lt;- tbl(conn, &#39;C_1990&#39;) |&gt; select(HEROP_ID, PovP1990=PovP, TotPop1990=TotPop) |&gt; collect() C_2000 &lt;- tbl(conn, &#39;C_2000&#39;) |&gt; select(HEROP_ID, PovP2000=PovP, TotPop2000=TotPop) |&gt; collect() changes &lt;- merge(C_2000, C_1990, on=&#39;HEROP_ID&#39;) |&gt; mutate(ChangeInPovP = PovP2000-PovP1990, ChangeInTotPop = TotPop2000-TotPop1990) |&gt; select(HEROP_ID, ChangeInPovP, ChangeInTotPop) |&gt; collect() head(changes) We can also interact with spatial data. This also requires the same hack as above: we cannot preview our results until after converting them to an sf object, at least within interactive R environments. In other words, with this connection to the spatial dataset: # Connect to the spatial dataset spatial_conn &lt;- dbConnect( bigquery(), project = &#39;oeps-391119&#39;, dataset = &#39;spatial&#39;, billing = billing ) This breaks: # breaks counties2010 &lt;- tbl(spatial_conn, &#39;counties2010&#39;) |&gt; collect() head(counties2010) and this works: # works counties2010 &lt;- tbl(spatial_conn, &#39;counties2010&#39;) |&gt; collect() |&gt; st_as_sf(wkt=&#39;geom&#39;, crs=&#39;EPSG:4326&#39;) head(counties2010) 5.4.1 A full dplyr pipeline: Putting all the pieces together, we can make our poverty map with the following code: # Make database connections spat_conn &lt;- dbConnect( bigquery(), project = &#39;oeps-391119&#39;, dataset = &#39;spatial&#39;, billing = billing ) tab_conn &lt;- dbConnect( bigquery(), project = &#39;oeps-391119&#39;, dataset = &#39;tabular&#39;, billing = billing ) # Grab tables C_1990 &lt;- tbl(tab_conn, &quot;C_1990&quot;) C_2000 &lt;- tbl(tab_conn, &quot;C_2000&quot;) counties2010 &lt;- tbl(spat_conn, &quot;counties2010&quot;) # Data wrangling C_1990 &lt;- C_1990 |&gt; select(HEROP_ID, PovP1990=PovP) |&gt; collect() C_2000 &lt;- C_2000 |&gt; select(HEROP_ID, PovP2000=PovP) |&gt; collect() change_in_pov &lt;- merge(C_2000, C_1990, on=&#39;HEROP_ID&#39;) |&gt; mutate(ChangeInPovP=PovP2000-PovP1990) |&gt; select(HEROP_ID, ChangeInPovP) counties2010 &lt;- counties2010 |&gt; collect() |&gt; st_as_sf(wkt=&#39;geom&#39;, crs=&#39;EPSG:4326&#39;) change_in_pov &lt;- merge(counties2010, change_in_pov, on=&#39;HEROP_ID&#39;) tm_shape(results) + tm_fill(&#39;ChangeInPovP&#39;, style=&#39;sd&#39;, midpoint=0, title=&#39;Change in Poverty\\n1990 to 2000&#39;, palette=&#39;-RdYlBu&#39;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
